{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx #to read docx files\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk #import Natural Language Toolkit to work with human language data\n",
    "import string\n",
    "exclude = set(string.punctuation) #punctuations that will be removed in the text\n",
    "exclude.update(\"’\", \"´\", \"…\", \"–\") #add \"’\", in some interviews this is used for it’s, it´s\n",
    "exclude.remove('_') #I use '_' to connect words that should be taken together, e.g. 'natural gas'\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer() #converting words into their base form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key text.latex.preview in file c:\\users\\mann\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key mathtext.fallback_to_cm in file c:\\users\\mann\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key savefig.jpeg_quality in file c:\\users\\mann\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key keymap.all_axes in file c:\\users\\mann\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key animation.avconv_path in file c:\\users\\mann\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key animation.avconv_args in file c:\\users\\mann\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt #for plots, e.g. of wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.util import ngrams #to check for n-grams\n",
    "import collections #to sort list by occurrences\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set size of text for other notebooks\n",
    "var_text_size = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the clean text from the other notebook\n",
    "%run ./p_01_import_transcripts_paper.ipynb\n",
    "# dict_all_interviews_qa: questions and answers combined. this dictionary contains all the interviews and their names (keys), e.g. 'UK_05_poli'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare dataframe with all interviews & paragraphs & country & classification. \n",
    "#Afterwards: merge with the df_dominant_topic_country on the text. \n",
    "df_all_interviews = pd.DataFrame()\n",
    "for y in dict_all_interviews_qa: #for each interview\n",
    "    var_key_split = y.split('_')\n",
    "    var_country = var_key_split[0]\n",
    "    var_number = var_key_split[1]\n",
    "    var_class = var_key_split[2]\n",
    "    for z in range(0, len(dict_all_interviews_qa[y].paragraphs)):\n",
    "        var_text = dict_all_interviews_qa[y].paragraphs[z].text\n",
    "        df_all_interviews = df_all_interviews.append({'Country': var_country, 'no_interview':var_number, \\\n",
    "                                 'class':var_class, 'Text':var_text}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5392"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_all_interviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn the individual interviews into one long list, each paragraph is one element in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_list_paras():\n",
    "    doc_list_paras = []\n",
    "    #dict_all_interviews = get_dict_all_interviews()\n",
    "    for x in dict_all_interviews_qa:\n",
    "        for y in dict_all_interviews_qa[x].paragraphs:\n",
    "            var_paragraph_text = dict_all_interviews_qa[x].paragraphs[y].text\n",
    "            if clean(var_paragraph_text) != '': #check if it has ONLY stopwords. If so, exclude!\n",
    "                doc_list_paras = doc_list_paras+var_paragraph_text\n",
    "    return(doc_list_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine stopwords: stopwords are a list of words that are very very common but don’t provide useful information\n",
    "stop = set(stopwords.words('english'))\n",
    "#add words that are either overlapping between wordclouds/topics and/or irrelevant\n",
    "stop |= set(['thats', 'whats', 'lets', 'would', 'wouldnt', 'could', 'couldnt', 'cannot', 'dont',\\\n",
    "             'yeah', 'yea', 'okay', 'yes', 'jaja', 'um', 'no', 'oh', 'ok',\\\n",
    "             'im', 'shes', 'hes', 'ive', 'ill', 'itll', 'youll', 'theyll', 'well', 'will', 'youre', 'theyre','u',\\\n",
    "             'theyd','cant', 'wont', 'doesnt', 'didnt', 'weve',\\\n",
    "             'think','know', 'going','say', 'get', 'want', 'come', 'go', 'see', 'said', 'put', 'let', 'got',\\\n",
    "             'tell', 'gonna','drove', 'getting', 'make', 'look', 'mean',\\\n",
    "             'lot', 'kind', 'there',  'also',  'one', 'course',  'like','actually', 'yet', 'issue',\\\n",
    "             'always', 'really', 'thing', 'obviously', 'even', 'maybe', 'something',  'already', '______',\\\n",
    "             'quite', 'anyway', 'blah', 'bla', 'fe', 'ta', 'ah', 'haha',\\\n",
    "            'qstart', 'qend'])  #'–' is in exclude!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'it' & 'there' is in stopwords, but still appears later in the topics. WHy?\n",
    "# 'thats', 'that', 'and', \"I'm\" etc.! \n",
    "# -->I changed order of punc & stop and added words to stopwords above!\n",
    "def clean(doc):\n",
    "    try_punc_free = \"\".join(ch for ch in doc.lower() if ch not in exclude)\n",
    "    try_stop_free = ' '.join([i for i in try_punc_free.split() if i not in stop])\n",
    "    try_normalized = \" \".join([lemma.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] \\\n",
    "                            else lemma.lemmatize(i) for i,j in nltk.pos_tag(nltk.word_tokenize(try_stop_free))])\n",
    "    #old: try_normalized = \" \".join(lemma.lemmatize(word) for word in try_stop_free.split()) # base or dictionary form of a word\n",
    "    \n",
    "    try_stop_free_2 = \" \".join([i for i in try_normalized.split() if i not in stop]) #do again, because lemmatized words might be stop words\n",
    "    return(try_stop_free_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a long list, each paragraph (document) is one item\n",
    "def get_doc_list_paras():\n",
    "    doc_list_paras = []\n",
    "    for x in dict_all_interviews_qa:\n",
    "        for y in range(0, len(dict_all_interviews_qa[x].paragraphs)):\n",
    "            var_paragraph_text = dict_all_interviews_qa[x].paragraphs[y].text\n",
    "            if clean(var_paragraph_text) != '': #check if it has ONLY stopwords\n",
    "                doc_list_paras.append(var_paragraph_text)\n",
    "    return(doc_list_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list_paras = get_doc_list_paras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5391"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_list_paras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check for n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### this part does not have to be run every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some words held meaning when they appear contiguously, like 'climate change' or 'ministry of energy'\n",
    "#we want to keep those together by adding a '_', i.e. 'climate_change'\n",
    "#can additionally use code from: https://nicharuc.github.io/topic_modeling/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#join all documents into one string\n",
    "doc_one_string = doc_list_paras.copy()\n",
    "doc_one_string = \"\".join(doc_one_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#create list with all n-grams for 2, 3, 4, 5 words\n",
    "list_n = [2,3,4]\n",
    "dict_lists_n_grams = {}\n",
    "for x in list_n:\n",
    "    var_str = doc_one_string.lower()\n",
    "    var_str = re.sub(r'[^a-zA-Z0-9\\var_str]', ' ', var_str) # exclude all non alphanumeric characters\n",
    "    tokens = [token for token in var_str.split(\" \") if token != \"\"]\n",
    "    output = list(ngrams(tokens, x)) #3,4,5\n",
    "    output_counter = Counter(output)\n",
    "    #create list with the n-grasm sorted by appearance. Only if more than 25 appearances.\n",
    "    dict_lists_n_grams[str(x)+'_grams'] = Counter({k: c for k, c in output_counter.items() if c >= 25}).most_common()\n",
    "    # collections. xxx \n",
    "    #drop n-grams that appear less than 20 times\n",
    "    #for y in dict_lists_n_grams:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#drop some n-grams from list\n",
    "dict_lists_n_grams_nouns = {}\n",
    "for x in dict_lists_n_grams:\n",
    "    dict_lists_n_grams_nouns[x] = []\n",
    "    list_NN = ['NN', 'NNS'] #JJ Adjective. #NN Noun, Singular. #NNS Noun Plural\n",
    "    #for 2-grams: first word should be adjective or noun, second noun.\n",
    "    if x == '2_grams':\n",
    "        for y in range(0, len(dict_lists_n_grams[x])):\n",
    "            words = dict_lists_n_grams[x][y][0]\n",
    "            tag = nltk.pos_tag(words)\n",
    "            if tag[0][1] in ['JJ', 'NN', 'NNS'] and tag[1][1] in list_NN:\n",
    "                #drop words that are only 't','i' or 's'\n",
    "                if 't' not in words and 'i' not in words and 's' not in words:\n",
    "                    dict_lists_n_grams_nouns[x].append(dict_lists_n_grams[x][y])\n",
    "    #for 3-grams: at least one word should be noun\n",
    "    if x == '3_grams':\n",
    "        for y in range(0, len(dict_lists_n_grams[x])):\n",
    "            words = dict_lists_n_grams[x][y][0]\n",
    "            tag = nltk.pos_tag(words)\n",
    "            #first word should be adjective or noun, second noun\n",
    "            if (tag[0][1] in list_NN or tag[1][1] in list_NN or tag[2][1] in list_NN):\n",
    "                if 't' not in words and 'i' not in words and 's' not in words:\n",
    "                    dict_lists_n_grams_nouns[x].append(dict_lists_n_grams[x][y])\n",
    "    #for 4-grams:at least one word should be noun\n",
    "    if x == '4_grams':\n",
    "        for y in range(0, len(dict_lists_n_grams[x])):\n",
    "            words = dict_lists_n_grams[x][y][0]\n",
    "            tag = nltk.pos_tag(words)\n",
    "            #first word should be adjective or noun, second noun\n",
    "            if (tag[0][1] in list_NN or tag[1][1] in list_NN or tag[2][1] in list_NN or tag[3][1] in list_NN):\n",
    "                if 't' not in words and 'i' not in words and 's' not in words:\n",
    "                    dict_lists_n_grams_nouns[x].append(dict_lists_n_grams[x][y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### this part has to be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually check the entire lists and extract words\n",
    "#from tetragrams\n",
    "list_tetragrams = ['coal fired power plants', 'coal fired power plant', 'ministry of the environment',\\\n",
    "'carbon capture and storage', 'committee on climate change', \\\n",
    "'coal fired power stations', 'coal fired power station']\n",
    "\n",
    "#form trigrams\n",
    "list_trigrams = ['coal phase out', 'ministry of energy', 'point of view', 'coal power plant',\\\n",
    "'state owned enterprise', 'net zero target', 'ministry of finance', 'feed in tariff', \\\n",
    "'department of energy', 'ministry of economics', 'power purchase agreements', 'power purchase agreement', \\\n",
    "'coal exit act', 'coal power plants', 'coal power plant', 'security of supply','ministry of environment',\\\n",
    "'climate change act', 'ministry of mines', 'power development plan', 'access to electricity',\\\n",
    "'clean power plan', 'environmental impact assessment', 'renewable energy act', \\\n",
    "'national development plan', 'energy regulatory commission', 'independent power producers',\\\n",
    "'state owned enterprises', 'minister of energy', 'fridays for future']\n",
    "\n",
    "#from bigrams\n",
    "list_bigrams = ['renewable energy', 'climate change', 'phase out', 'power plant', 'power plants', 'natural gas',\\\n",
    "'energy sector', 'coal fired', 'net zero', 'coal power', 'coal plants', 'coal plant', 'coal commission',\\\n",
    "'energy efficiency', 'energy policy', 'carbon price', 'private sector', 'long term', 'climate protection',\\\n",
    "'prime minister', 'power stations', 'state owned', 'climate policy', 'power sector', 'carbon pricing',\\\n",
    "'united states', 'world bank', 'coal exit', 'low carbon', 'structural change', 'shut down',\\\n",
    "'energy transition', 'renewable energies', 'fossil fuel', 'energy act', 'fossil fuels', 'development plan',\\\n",
    "'civil society', 'energy mix', 'eu ets', 'energy security', 'power plan', 'emissions trading', 'energy system',\\\n",
    "'paris agreement', 'electricity generation', 'power station', 'electricity price', 'capacity market',\\\n",
    "'trade unions', 'electricity market', 'hambach forest', 'off grid', 'price floor', 'gas prices', 'phasing out',\\\n",
    "'carbon capture', 'clean coal', 'economic growth',\\\n",
    "'federal government', 'local government', 'power generation', 'coal mining', 'offshore wind', 'federal states',\\\n",
    " 'know how', 'electricity prices', 'conservative party', 'environmental associations', 'central government',\\\n",
    " 'economic development', 'national government', 'south africa', 'solar panels', 'political economy', \\\n",
    "'air pollution', 'electric vehicles', 'think tanks', 'think tank', 'greenhouse gas', 'carbon neutrality', 'air quality', \\\n",
    "'net metering', 'base load', 'just transition', 'least cost', 'price floor', 'trade union',\\\n",
    "'extinction rebellion']\n",
    "\n",
    "dict_n_to_replace = {'4_grams':list_tetragrams, '3_grams':list_trigrams, '2_grams':list_bigrams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the words should be connected with a '_'. Some words however are plural and should be replace by singular.l\n",
    "dict_n_lists_automatically = {}\n",
    "for x in dict_n_to_replace:\n",
    "    list_replacements = list()\n",
    "    for y in dict_n_to_replace[x]:\n",
    "        list_replacements.append(y.replace(' ', '_'))\n",
    "        dict_n_lists_automatically[x] = list_replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change manually, e.g. plural to singular!\n",
    "list_tetragrams_replacements = ['coal_fired_power_plant', 'coal_fired_power_plant',\\\n",
    " 'ministry_of_the_environment', 'carbon_capture_and_storage', 'committee_on_climate_change',\\\n",
    " 'coal_fired_power_station', 'coal_fired_power_station']\n",
    "\n",
    "list_trigrams_replacements = ['coal_phase_out', 'ministry_of_energy', 'point_of_view', 'coal_power_plant',\\\n",
    " 'state_owned_enterprise', 'net_zero_target', 'ministry_of_finance', 'feed_in_tariff', \\\n",
    " 'department_of_energy', 'ministry_of_economics', 'power_purchase_agreement', 'power_purchase_agreement',\\\n",
    " 'coal_exit_act', 'coal_power_plant', 'coal_power_plant', 'security_of_supply', 'ministry_of_environment',\\\n",
    " 'climate_change_act', 'ministry_of_mines', 'power_development_plan', 'access_to_electricity',\\\n",
    " 'clean_power_plan', 'environmental_impact_assessment', 'renewable_energy_act', 'national_development_plan',\\\n",
    " 'energy_regulatory_commission', 'independent_power_producers', 'state_owned_enterprise',\\\n",
    " 'minister_of_energy', 'fridays_for_future']\n",
    "\n",
    "list_bigrams_replacements = ['renewable_energy', 'climate_change', 'phase_out', 'power_plant', 'power_plant',\\\n",
    " 'natural_gas', 'energy_sector', 'coal_fired', 'net_zero', 'coal_power', 'coal_plant', 'coal_plant',\\\n",
    " 'coal_commission', 'energy_efficiency', 'energy_policy', 'carbon_price', 'private_sector', 'long_term',\\\n",
    " 'climate_protection', 'prime_minister', 'power_station', 'state_owned', 'climate_policy', 'power_sector',\\\n",
    " 'carbon_pricing', 'united_states', 'world_bank', 'coal_exit', 'low_carbon', 'structural_change',\\\n",
    " 'shut_down', 'energy_transition', 'renewable_energy', 'fossil_fuel', 'energy_act', 'fossil_fuel',\\\n",
    " 'development_plan', 'civil_society', 'energy_mix', 'eu_ets', 'energy_security', 'power_plan',\\\n",
    " 'emissions_trading', 'energy_system', 'paris_agreement', 'electricity_generation', 'power_station',\\\n",
    " 'electricity_price', 'capacity_market', 'trade_union', 'electricity_market', 'hambach_forest',\\\n",
    " 'off_grid', 'price_floor', 'gas_price', 'phasing_out', 'carbon_capture', 'clean_coal', 'economic_growth',\\\n",
    " 'federal_government', 'local_government', 'power_generation', 'coal_mining', 'offshore_wind', 'federal_states',\\\n",
    " 'know_how', 'electricity_price', 'conservative_party', 'environmental_association', 'central_government',\\\n",
    " 'economic_development', 'national_government', 'south_africa', 'solar_panel', 'political_economy',\\\n",
    " 'air_pollution', 'electric_vehicles', 'think_tank', 'think_tank', 'greenhouse_gas', 'carbon_neutrality',\\\n",
    " 'air_quality', 'net_metering', 'base_load', 'just_transition', 'least_cost', 'price_floor', 'trade_union', \\\n",
    " 'extinction_rebellion']\n",
    "\n",
    "dict_n_replacements = {'4_grams':list_tetragrams_replacements, '3_grams':list_trigrams_replacements, \\\n",
    "            '2_grams':list_bigrams_replacements}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionaries, the keys are the original words, the values the new combined words\n",
    "dict_tetragrams = dict(zip(dict_n_to_replace['4_grams'], dict_n_replacements['4_grams']))\n",
    "dict_trigrams = dict(zip(dict_n_to_replace['3_grams'], dict_n_replacements['3_grams']))\n",
    "dict_bigrams = dict(zip(dict_n_to_replace['2_grams'], dict_n_replacements['2_grams']))\n",
    "#create one list including all three\n",
    "dict_list = [dict_tetragrams, dict_trigrams, dict_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace the n-grams in each document:\n",
    "doc_list_paras_n = doc_list_paras.copy()\n",
    "for d in dict_list:\n",
    "    for x in range(0, len(doc_list_paras_n)):\n",
    "        for y in d.keys():\n",
    "            if y in doc_list_paras_n[x].lower():\n",
    "                doc_list_paras_n[x] = re.sub(y, d[y], doc_list_paras_n[x], flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### strip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the punctuations, stopwords and normalize the corpus\n",
    "def get_doc_clean():\n",
    "    doc_clean = [clean(doc).split() for doc in doc_list_paras_n]\n",
    "    return(doc_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean = get_doc_clean() #doc_clean contains n-grams!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_countries = ['DE', 'UK', 'CL', 'CO', 'US', 'KE', 'ZA', 'ID', 'IN', 'VN', 'PH', 'PK'] #order from clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the wordclouds & high-frequency words:\n",
    "#For each country: each reply (so each paragraph) is one text:\n",
    "def get_dict_country_doc_list_paras_n_clean():\n",
    "    #do one list per country (for wordcloud)\n",
    "    dict_country_doc_list_paras = {}\n",
    "    dict_country_doc_clean_list_paras = {}\n",
    "    dict_country_doc_list_paras_n = {}\n",
    "    dict_country_doc_list_paras_n_clean = {}\n",
    "    for x in set_countries:\n",
    "        #add all paragraphs into a list:\n",
    "        dict_country_doc_list_paras[x] = []\n",
    "        for y in dict_all_interviews_qa:\n",
    "            if x in y: #if country abbreviation is in the interview title\n",
    "                dict_country_doc_list_paras[x] = dict_country_doc_list_paras[x]+([p.text for p in dict_all_interviews_qa[y].paragraphs if p.text])\n",
    "        #replace the n-grams in each document:\n",
    "        dict_country_doc_list_paras_n[x] = dict_country_doc_list_paras[x].copy()\n",
    "        for d in dict_list: #dict_list contains all the n-grams\n",
    "            for n in range(0, len(dict_country_doc_list_paras_n[x])):\n",
    "                for k in d.keys():\n",
    "                    if k in dict_country_doc_list_paras_n[x][n].lower(): #if n-gram in there, replace!\n",
    "                        dict_country_doc_list_paras_n[x][n] = re.sub(k, d[k], dict_country_doc_list_paras_n[x][n], flags=re.IGNORECASE)\n",
    "\n",
    "        #clean the data\n",
    "        dict_country_doc_list_paras_n_clean[x] = [clean(doc).split() for doc in dict_country_doc_list_paras_n[x]]\n",
    "    return(dict_country_doc_list_paras_n_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_country_doc_list_paras_n_clean = get_dict_country_doc_list_paras_n_clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do versions with only long docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_wordcount_drop = [0, 1, 2, 5, 7, 10, 20, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, need to link clean docs to original documents\n",
    "#doc_list_paras contains all the original texts\n",
    "#doc_clean all the cleaned documents, so only words (without stopwords etc.)\n",
    "df_paras_and_clean = pd.DataFrame({'Text':doc_list_paras, 'Clean_words':doc_clean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the words\n",
    "#add column with the number of words ('num_words_clean)\n",
    "df_paras_and_clean_count = df_paras_and_clean.copy()\n",
    "for x in df_paras_and_clean_count.index:\n",
    "    df_paras_and_clean_count.loc[x,'num_words_clean'] = \\\n",
    "            len(df_paras_and_clean_count.loc[x, 'Clean_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create doc_list_paras and doc_clean for each of the x (word limit to drop)\n",
    "dict_doc_list_paras_x = {}\n",
    "dict_clean_doc_x = {}\n",
    "for x in list_wordcount_drop:\n",
    "    \n",
    "    list_paras_x = []\n",
    "    for y in df_paras_and_clean_count.index:\n",
    "        if df_paras_and_clean_count.loc[y, 'num_words_clean'] > x:\n",
    "            list_paras_x.append(df_paras_and_clean_count.loc[y, 'Text'])\n",
    "    dict_doc_list_paras_x['>'+str(x)] = list_paras_x\n",
    "    \n",
    "    list_x_docs = []\n",
    "    for z in doc_clean:\n",
    "        if len(z) > x :\n",
    "            list_x_docs.append(z)\n",
    "    dict_clean_doc_x['>'+str(x)] = list_x_docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
