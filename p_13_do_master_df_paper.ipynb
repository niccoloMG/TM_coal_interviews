{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time #to have today's date\n",
    "timestr = time.strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "\n",
      "Bad key text.latex.preview in file c:\\users\\mann\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key mathtext.fallback_to_cm in file c:\\users\\mann\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key savefig.jpeg_quality in file c:\\users\\mann\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key keymap.all_axes in file c:\\users\\mann\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key animation.avconv_path in file c:\\users\\mann\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key animation.avconv_args in file c:\\users\\mann\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "#get the LDA model from another notebook\n",
    "%run ./p_07_final_LDA_paper.ipynb\n",
    "\n",
    "#doc_term_matrix_x \n",
    "#ldamodel =\n",
    "#var_drop_x #drop all documents with number of words <= x. Only keep documents >x\n",
    "#doc_clean_x \n",
    "#doc_list_paras_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do dataframe with all the topics+categories+keywords and the text+questions per document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe with empty columns\n",
    "topics_per_doc_df = pd.DataFrame()\n",
    "list_topics_columns = list() #to use later\n",
    "for x in range(0,var_no_topics):\n",
    "    topics_per_doc_df[\"perc_cont_\"+str(x)] = []\n",
    "    topics_per_doc_df[\"keywords_\"+str(x)] = []\n",
    "    list_topics_columns.append(\"perc_cont_\"+str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-52be923bf62e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Get relevant topics for each document, this takes a while\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc_term_matrix_x\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#this provides a list with all topics and their percentage contribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;31m# Get all the topics, Perc Contribution and Keywords for each document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtopic_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mann\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\gensim\\interfaces.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocno\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mann\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, bow, eps)\u001b[0m\n\u001b[0;32m   1546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1547\u001b[0m         \"\"\"\n\u001b[1;32m-> 1548\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_document_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimum_phi_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'state'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dispatcher'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseparately\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mann\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mget_document_topics\u001b[1;34m(self, bow, minimum_probability, minimum_phi_value, per_word_topics)\u001b[0m\n\u001b[0;32m   1351\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1353\u001b[1;33m         \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1354\u001b[0m         \u001b[0mtopic_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# normalize distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mann\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[1;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[0;32m    716\u001b[0m                 \u001b[1;31m# Substituting the value of the optimal phi back into\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m                 \u001b[1;31m# the update for gamma gives this update. Cf. Lee&Seung 2001.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 718\u001b[1;33m                 \u001b[0mgammad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mexpElogthetad\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcts\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mphinorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpElogbetad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    719\u001b[0m                 \u001b[0mElogthetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgammad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[0mexpElogthetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mElogthetad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get relevant topics for each document, this takes a while\n",
    "for i, row in enumerate(ldamodel[doc_term_matrix_x]): #this provides a list with all topics and their percentage contribution\n",
    "    # Get all the topics, Perc Contribution and Keywords for each document\n",
    "    for j in row:\n",
    "        topic_num = j[0]\n",
    "        prop_topic = j[1]\n",
    "        wp = ldamodel.show_topic(topic_num)\n",
    "        topic_keywords = \", \".join([word for word, prop in wp])\n",
    "        topics_per_doc_df.loc[i, \"perc_cont_\"+str(topic_num)] = prop_topic\n",
    "        topics_per_doc_df.loc[i, \"keywords_\"+str(topic_num)] = topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add original text to the end of the output\n",
    "contents = pd.Series(doc_list_paras_x)\n",
    "topics_per_doc_df = pd.concat([topics_per_doc_df, contents], axis=1)\n",
    "topics_per_doc_df.rename(columns={0:'Text'}, inplace=True) #rename the added column with the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no need anymore, question is already in text\n",
    "#if available, also add the question that was asked to each document\n",
    "#df_questions_replies = pd.read_csv('.//results//topics_group//questions_replies_'+timestr+'_V01.csv',\n",
    "#    sep=';', decimal=',', index_col=0)\n",
    "df_docs_topics = topics_per_doc_df#.merge(df_questions_replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge to get information on country, class etc. for each document\n",
    "df_docs_topics_country_class = df_all_interviews.merge(df_docs_topics) #do I need df_all_interviews_x? -->no!\n",
    "#P: the text \"I don't have information on that.\" is said two times.\n",
    "#thus, when we merge the dataframe, it appears four times. \n",
    "#S: drop duplicate rows:\n",
    "df_docs_topics_country_class = df_docs_topics_country_class.drop_duplicates().\\\n",
    "        reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs_topics_country_class.to_csv('.//results//master_dfs//docs_oldNum_topics_country_class_keywords_text_q_'+timestr+'_V01.csv',\n",
    "    sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## change numbers to new topic numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dataframe with the categories (numbers & names), topics (numbers (ori&new) & names), and keywords and lambda\n",
    "df_topics_categories = pd.read_csv\\\n",
    "('.//results//master_dfs//category_numname_topics_numname_orinew_keywords_lambda_'+timestr+'_V01.csv',\n",
    "    sep=';', decimal=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_nums = df_topics_categories[['ori_topic_num', 'Topic_num']].sort_values('ori_topic_num')\n",
    "#the keys are the old (original) topic numbers, the value are the new ones.\n",
    "dict_t_nums = dict(zip(df_topic_nums.ori_topic_num, df_topic_nums.Topic_num))\n",
    "#so for all the keys (for x in dict_t_nums), if x is the number, change x to dict_t_nums[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the numbers\n",
    "df_docs_newNum_topics_country_class = df_docs_topics_country_class.copy()\n",
    "\n",
    "for x in df_docs_newNum_topics_country_class.columns:\n",
    "    list_x_split = x.split('_')\n",
    "    if list_x_split[0] == 'perc': #so column is 'perc_cont_x'\n",
    "        var_number = int(list_x_split[2])\n",
    "        df_docs_newNum_topics_country_class.rename(columns = {x:'t_'+str(dict_t_nums[var_number])+'_perc_cont'}, inplace = True)\n",
    "    if list_x_split[0] == 'keywords': #so column is 'perc_cont_x'\n",
    "        var_number = int(list_x_split[1])\n",
    "        df_docs_newNum_topics_country_class.rename(columns = {x:'t_'+str(dict_t_nums[var_number])+'_keywords'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rearrange column names:\n",
    "list_perc = []\n",
    "for x in range(1, df_topic_nums['ori_topic_num'].max()+2):\n",
    "    list_perc.append('t_'+str(x)+'_perc_cont')\n",
    "    list_perc.append('t_'+str(x)+'_keywords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_columns = ['Country', 'no_interview', 'class', 'Text'] + list_perc #+ ['Question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs_newNum_topics_country_class_order = df_docs_newNum_topics_country_class[list_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>no_interview</th>\n",
       "      <th>class</th>\n",
       "      <th>Text</th>\n",
       "      <th>t_1_perc_cont</th>\n",
       "      <th>t_1_keywords</th>\n",
       "      <th>t_2_perc_cont</th>\n",
       "      <th>t_2_keywords</th>\n",
       "      <th>t_3_perc_cont</th>\n",
       "      <th>t_3_keywords</th>\n",
       "      <th>...</th>\n",
       "      <th>t_18_perc_cont</th>\n",
       "      <th>t_18_keywords</th>\n",
       "      <th>t_19_perc_cont</th>\n",
       "      <th>t_19_keywords</th>\n",
       "      <th>t_20_perc_cont</th>\n",
       "      <th>t_20_keywords</th>\n",
       "      <th>t_21_perc_cont</th>\n",
       "      <th>t_21_keywords</th>\n",
       "      <th>t_22_perc_cont</th>\n",
       "      <th>t_22_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4983</th>\n",
       "      <td>VN</td>\n",
       "      <td>11</td>\n",
       "      <td>socint</td>\n",
       "      <td>&lt;q/start&gt;  do you think that for example the c...</td>\n",
       "      <td>0.136787</td>\n",
       "      <td>people, work, political, talk, time, community...</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>everyones, metro, insurance, expenditure, char...</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>job, labor, worker, trade_union, secretary, em...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131321</td>\n",
       "      <td>emission, climate_change, target, sector, carb...</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>net_zero, decarbonisation, geothermal, health,...</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>uk, year, gas, big, government, around, renewa...</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>challenge, goal, development, important, persp...</td>\n",
       "      <td>0.037822</td>\n",
       "      <td>coal, important, phaseout, question, policy, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4984</th>\n",
       "      <td>VN</td>\n",
       "      <td>11</td>\n",
       "      <td>socint</td>\n",
       "      <td>&lt;q/start&gt;  thank you so much, that was very in...</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>people, work, political, talk, time, community...</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>everyones, metro, insurance, expenditure, char...</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>job, labor, worker, trade_union, secretary, em...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>emission, climate_change, target, sector, carb...</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>net_zero, decarbonisation, geothermal, health,...</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>uk, year, gas, big, government, around, renewa...</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>challenge, goal, development, important, persp...</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>coal, important, phaseout, question, policy, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4985</th>\n",
       "      <td>VN</td>\n",
       "      <td>11</td>\n",
       "      <td>socint</td>\n",
       "      <td>&lt;q/start&gt;  yes because now I mean when you loo...</td>\n",
       "      <td>0.068875</td>\n",
       "      <td>people, work, political, talk, time, community...</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>everyones, metro, insurance, expenditure, char...</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>job, labor, worker, trade_union, secretary, em...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>emission, climate_change, target, sector, carb...</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>net_zero, decarbonisation, geothermal, health,...</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>uk, year, gas, big, government, around, renewa...</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>challenge, goal, development, important, persp...</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>coal, important, phaseout, question, policy, r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Country no_interview   class  \\\n",
       "4983      VN           11  socint   \n",
       "4984      VN           11  socint   \n",
       "4985      VN           11  socint   \n",
       "\n",
       "                                                   Text  t_1_perc_cont  \\\n",
       "4983  <q/start>  do you think that for example the c...       0.136787   \n",
       "4984  <q/start>  thank you so much, that was very in...       0.000657   \n",
       "4985  <q/start>  yes because now I mean when you loo...       0.068875   \n",
       "\n",
       "                                           t_1_keywords  t_2_perc_cont  \\\n",
       "4983  people, work, political, talk, time, community...       0.000221   \n",
       "4984  people, work, political, talk, time, community...       0.000657   \n",
       "4985  people, work, political, talk, time, community...       0.000158   \n",
       "\n",
       "                                           t_2_keywords  t_3_perc_cont  \\\n",
       "4983  everyones, metro, insurance, expenditure, char...       0.000221   \n",
       "4984  everyones, metro, insurance, expenditure, char...       0.000657   \n",
       "4985  everyones, metro, insurance, expenditure, char...       0.000158   \n",
       "\n",
       "                                           t_3_keywords  ...  t_18_perc_cont  \\\n",
       "4983  job, labor, worker, trade_union, secretary, em...  ...        0.131321   \n",
       "4984  job, labor, worker, trade_union, secretary, em...  ...        0.000657   \n",
       "4985  job, labor, worker, trade_union, secretary, em...  ...        0.000158   \n",
       "\n",
       "                                          t_18_keywords  t_19_perc_cont  \\\n",
       "4983  emission, climate_change, target, sector, carb...        0.000221   \n",
       "4984  emission, climate_change, target, sector, carb...        0.000657   \n",
       "4985  emission, climate_change, target, sector, carb...        0.000158   \n",
       "\n",
       "                                          t_19_keywords  t_20_perc_cont  \\\n",
       "4983  net_zero, decarbonisation, geothermal, health,...        0.000221   \n",
       "4984  net_zero, decarbonisation, geothermal, health,...        0.000657   \n",
       "4985  net_zero, decarbonisation, geothermal, health,...        0.000158   \n",
       "\n",
       "                                          t_20_keywords  t_21_perc_cont  \\\n",
       "4983  uk, year, gas, big, government, around, renewa...        0.000221   \n",
       "4984  uk, year, gas, big, government, around, renewa...        0.000657   \n",
       "4985  uk, year, gas, big, government, around, renewa...        0.000158   \n",
       "\n",
       "                                          t_21_keywords  t_22_perc_cont  \\\n",
       "4983  challenge, goal, development, important, persp...        0.037822   \n",
       "4984  challenge, goal, development, important, persp...        0.000657   \n",
       "4985  challenge, goal, development, important, persp...        0.000158   \n",
       "\n",
       "                                          t_22_keywords  \n",
       "4983  coal, important, phaseout, question, policy, r...  \n",
       "4984  coal, important, phaseout, question, policy, r...  \n",
       "4985  coal, important, phaseout, question, policy, r...  \n",
       "\n",
       "[3 rows x 48 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_docs_newNum_topics_country_class_order.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs_newNum_topics_country_class_order.to_csv('.//results//master_dfs//docs_newNum_topics_country_class_keywords_text_q_'+timestr+'_V01.csv',\n",
    "    sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
